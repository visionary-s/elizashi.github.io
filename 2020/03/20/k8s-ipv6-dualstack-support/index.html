<!DOCTYPE html>
<html lang="en">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Support ipv6/dualStack in K8s |
    
    Solito</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  <link rel="stylesheet" href="/css/style.css">
  
    <link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">
  
  <script src="/js/pace.min.js"></script>
</head>
</html>
<body>
<main class="content">
  <section class="outer">
  <article id="post-k8s-ipv6-dualstack-support" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Support ipv6/dualStack in K8s
    </h1>
  


      </header>
    

    
      <div class="article-meta">
        <a href="/2020/03/20/k8s-ipv6-dualstack-support/" class="article-date">
  <time datetime="2020-03-19T16:00:00.000Z" itemprop="datePublished">2020-03-20</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/work/">work</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <p><strong>记录惊天巨坑enable ipv6 and dual stack for our product，从k8s安装开始</strong></p>
<p><em>0. 原来产品是本地安装测试的，仅支持ipv4安装很简便，但这次要求支持ipv6/dualStack，根据<a href="https://kubernetes.io/docs/concepts/services-networking/dual-stack/" target="_blank" rel="noopener">官网文档</a>，我们需要1.16版本以上的kubernetes，<code>kubectl version</code>查看本地版本:</em></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Client Version: version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"14"</span>, GitVersion:<span class="string">"v1.14.8"</span>, GitCommit:<span class="string">"211047e9a1922595eaa3a1127ed365e9299a6c23"</span>, GitTreeState:<span class="string">"clean"</span>, BuildDate:<span class="string">"2019-10-15T12:11:03Z"</span>, GoVersion:<span class="string">"go1.12.10"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"windows/amd64"</span>&#125;</span><br><span class="line">Server Version: version.Info&#123;Major:<span class="string">"1"</span>, Minor:<span class="string">"14"</span>, GitVersion:<span class="string">"v1.14.8"</span>, GitCommit:<span class="string">"211047e9a1922595eaa3a1127ed365e9299a6c23"</span>, GitTreeState:<span class="string">"clean"</span>, BuildDate:<span class="string">"2019-10-15T12:02:12Z"</span>, GoVersion:<span class="string">"go1.12.10"</span>, Compiler:<span class="string">"gc"</span>, Platform:<span class="string">"linux/amd64"</span>&#125;</span><br></pre></td></tr></table></figure>

<p><em>1.14。。赶紧查docker desktop自带的kubernetes怎么更新，发现<a href="https://monkeywie.github.io/2019/09/16/k8s-update-operate/" target="_blank" rel="noopener">要写deploy</a>测试或者<a href="https://www.cnblogs.com/lonelyxmas/p/10670477.html" target="_blank" rel="noopener">重装k8s?</a> 感觉不适用。因为这是docker自带的不知道重装会不会有别的影响。决定直接装到我们组的remote server上，这样测试也可以一步到位。</em></p>
<center><a id="more"></a></center>

<ol>
<li>登上remote server发现第二个坑，机子连不了外网。。。连yum install都必须自己配置本地yum repo…一开始是想自己再装一个能连外网的虚拟机，把docker和k8s下载下来打好包再transfer到remote server上。</li>
</ol>
<ul>
<li>setup redhat7 to local vm: <br><em>error when setup vm</em>: <code>VT-x is not available (VERR_VMX_NO_VMX)</code><br><em>solution</em>: <a href="https://blog.csdn.net/imilano/article/details/83038682" target="_blank" rel="noopener">https://blog.csdn.net/imilano/article/details/83038682</a>  (note: this action will affect the auto-start of docker)</li>
<li>enable the subscription before downloading docker:<br><a href="https://blog.csdn.net/yl_1314/article/details/52044022" target="_blank" rel="noopener">https://blog.csdn.net/yl_1314/article/details/52044022</a></li>
</ul>
<ol start="2">
<li>不太行，改用这个: <a href="https://github.com/wxdlong/ok8s" target="_blank" rel="noopener">https://github.com/wxdlong/ok8s</a> <br>把包下到本地再放到60上</li>
</ol>
<ul>
<li>before downloading, need change user permission level on docker. run command with:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export MSYS_NO_PATHCONV=1</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>then add local user to group “docker-users”, “Hyper-V Administrators”,” Remote Desktop Users”,”Remote Management Users”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS H:\&gt; net localgroup docker-users ERICSSON\&lt;eid&gt; /add</span><br><span class="line">System error 1378 has occurred.</span><br><span class="line">The specified account name is already a member of the group.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PS H:\&gt; net localgroup &quot;Hyper-V Administrators&quot; ERICSSON\&lt;eid&gt; /add</span><br><span class="line">The command completed successfully.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PS H:\&gt; net localgroup &quot;Remote Desktop Users&quot; ERICSSON\&lt;eid&gt; /add</span><br><span class="line">System error 1378 has occurred.</span><br><span class="line">The specified account name is already a member of the group.</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">PS H:\&gt; net localgroup &quot;Remote Management Users&quot; ERICSSON\&lt;eid&gt; /add</span><br><span class="line">The command completed successfully.</span><br></pre></td></tr></table></figure>

<ul>
<li><p>check shared docker in DockerDesktop<br><img src="https://res.cloudinary.com/elizashi/image/upload/v1584431386/gitblog/posts/linux-docker-k8s/docker-desktop-share_p0z97p.png" alt="alt"><br><em>If still can’t see the volume, relogon PC, sec, and reset credentials.</em></p>
</li>
<li><p>download packages to local folder <code>download</code></p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run --rm -v <span class="string">'//c//Users//&lt;eid&gt;//download:/ok8s'</span> registry.cn-hangzhou.aliyuncs.com/wxdlong/ok8s:v1.16.3</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>接下来就是把这个folder copy到host server上按教程往下走。</p>
<ol start="2">
<li>配置k8s和docker过程中遇到的<strong>错误</strong></li>
</ol>
<ul>
<li><p><code>kubectl cluster-info</code> return <br>[error] <code>The connection to the server localhost:8080 was refused - did you specify the right host or port?</code></p>
<ul>
<li><a href="https://blog.csdn.net/wzygis/article/details/91354870" target="_blank" rel="noopener">https://blog.csdn.net/wzygis/article/details/91354870</a><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"export KUBECONFIG=/etc/kubernetes/admin.conf"</span> &gt;&gt; ~/.bash_profile</span><br><span class="line"><span class="built_in">source</span> ~/.bash_profile</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>包里缺少<strong>flannel插件</strong>,自行安装</p>
<ul>
<li><p>Copy <code>/etc/kubernetes/admin.conf</code> to <code>$HOME/.kube/</code>(on windows, on host server we just use <code>/root</code>)：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ~]# mkdir -p /root/.kube</span><br><span class="line">[root@192-168-1-61 .kube]# cp -i /etc/kubernetes/admin.conf /root/.kube/config</span><br><span class="line">[root@192-168-1-61 .kube]# ls -ltr /root/.kube/</span><br><span class="line">total 8</span><br><span class="line">  -rw-------. 1 root root 5448 Mar 17 21:33 config</span><br></pre></td></tr></table></figure>

<p><em>这一步可能是多余的，因为上面第一个错误已经export过路径了</em></p>
<p>check pod status</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ok8s]# kubectl get pods -A</span><br><span class="line">NAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   coredns-5644d7b6d9-5sp86                    0/1     Pending   0          15h</span><br><span class="line">kube-system   coredns-5644d7b6d9-qjfz9                    0/1     Pending   0          15h</span><br><span class="line">kube-system   etcd-192-168-1-61.maas                      1/1     Running   0          15h</span><br><span class="line">kube-system   kube-apiserver-192-168-1-61.maas            1/1     Running   0          15h</span><br><span class="line">kube-system   kube-controller-manager-192-168-1-61.maas   1/1     Running   0          15h</span><br><span class="line">kube-system   kube-flannel-ds-amd64-vx4bw                 1/1     Running   0          15h</span><br><span class="line">kube-system   kube-proxy-h4gdc                            1/1     Running   0          15h</span><br><span class="line">kube-system   kube-scheduler-192-168-1-61.maas            1/1     Running   0          15h</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<p>According to above result, the core DNS service is not started successfully.</p>
<ul>
<li>flannel still not work, check <code>tail -f /var/log/messages</code> found <code>Unable to update cni config: no valid networks found in /etc/cni/net.d</code> + <code>[fork/exec /opt/ok8s/cni/flannel: permission denied fork/exec /opt/ok8s/cni/portmap: permission denied]</code> <br>go to folder <code>cd /opt/ok8s/cni/</code> and run <code>chmod +x *</code>. <br><em>tip: 无权限文件一般是白字，像上面这样授予全部权限会变为绿色，chmod 777 会把文件变为灰底</em></li>
<li>再次<code>kubectl get pods -A</code>可以看到两个core-dns node已经跑起来了</li>
</ul>
<ul>
<li><strong>安装dashboard</strong><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ok8s]# kubectl get pods -A</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-76585494d8-qb5mw   1/1     Running            0          101m</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-5996555fd8-88qz6        0/1     ImagePullBackOff   0          101m</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>这里pull image失败了，看了下应该是因为host server不能连外网，dashboard没大用处，这步暂时跳过</p>
<ul>
<li><p>更换network插件为<strong>calico</strong> <br>装完flannel发现它还<a href="https://github.com/coreos/flannel/issues/248" target="_blank" rel="noopener">没支持ipv6</a>，我人傻了，只好重新安装calico，先存一下calico支持<a href="https://docs.projectcalico.org/networking/ipv6" target="_blank" rel="noopener">ipv6</a> | <a href="https://docs.projectcalico.org/networking/dual-stack" target="_blank" rel="noopener">dual-stack</a>的官方文档, 以及<a href="https://shouneng.website/2019/08/23/zai-kubernetes-zhong-zheng-que-di-an-zhuang-calico/" target="_blank" rel="noopener">非常详细的中文安装教程</a></p>
<ul>
<li><p><code>kubeadm reset</code>后更改pod-network-cidr，重新init cluster:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ok8s]# kubeadm init --v=7 --pod-network-cidr=192.168.0.0/16 --kubernetes-version=v1.16.3</span><br></pre></td></tr></table></figure>

<blockquote>
<p>Calico作为CNI插件安装。必须通过传递–network-plugin=cni参数将kubelet配置为使用CNI网络, 这里–pod-network-cidr=192.168.0.0就是是用来给 controller-manager 用作自动分配pod子网 (用作给每个node上的pod分配IP address)</p>
</blockquote>
</li>
<li><p>follow this <a href="https://docs.projectcalico.org/getting-started/kubernetes/quickstart" target="_blank" rel="noopener">official doc</a></p>
</li>
<li><p>由于不能连外网导致image pull不下来，手动下载过去</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-system   calico-node-vcbmc     0/1     Init:ImagePullBackOff   0     9m12s</span><br></pre></td></tr></table></figure>

</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 nodeagent~uds]# kubectl describe pods calico-node-vcbmc -n kube-system</span><br><span class="line">Events:</span><br><span class="line">Type     Reason     Age                     From                        Message</span><br><span class="line">Normal   Scheduled  9m44s                   default-scheduler           Successfully assigned kube-system/calico-node-vcbmc to 192-168-1-61.maas</span><br><span class="line">Warning  Failed     9m3s                    kubelet, 192-168-1-61.maas  Failed to pull image &quot;calico/cni:v3.13.1&quot;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: dial tcp: lookup registry-1.docker.io on 10.136.40.87:53: server misbehaving</span><br><span class="line">Normal   Pulling    7m37s (x4 over 9m44s)   kubelet, 192-168-1-61.maas  Pulling image &quot;calico/cni:v3.13.1&quot;</span><br><span class="line">Warning  Failed     7m22s (x3 over 9m29s)   kubelet, 192-168-1-61.maas  Failed to pull image &quot;calico/cni:v3.13.1&quot;: rpc error: code = Unknown desc = Error response from daemon: Get https://registry-1.docker.io/v2/: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)</span><br><span class="line">Warning  Failed     7m22s (x4 over 9m29s)   kubelet, 192-168-1-61.maas  Error: ErrImagePull</span><br><span class="line">Warning  Failed     7m9s (x6 over 9m28s)    kubelet, 192-168-1-61.maas  Error: ImagePullBackOff</span><br><span class="line">Normal   BackOff    4m36s (x16 over 9m28s)  kubelet, 192-168-1-61.maas  Back-off pulling image &quot;calico/cni:v3.13.1&quot;</span><br></pre></td></tr></table></figure>

<p>download <code>calico:v3.13.1</code> via <a href="https://docs.projectcalico.org/release-notes/" target="_blank" rel="noopener">https://docs.projectcalico.org/release-notes/</a>, transfer the tgz package to host server. <br><code>[root@192-168-1-61 calico]# docker load --input /home/eshibij/calico-v3.13.1/release-v3.13.1/images/calico-node.tar</code> –&gt; load calico image, check:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 calico]# docker images</span><br><span class="line">REPOSITORY                           TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">calico/node                          v3.13.1             2e5029b93d4a        5 days ago          260MB</span><br></pre></td></tr></table></figure>

<p>把<code>pod2daemon-flexvol</code>, <code>calico-cni</code>, <code>calico-kube-controllers</code>也一起load了</p>
<ul>
<li><strong>[error]</strong> pod describe find <code>calico 0/1 nodes are available: 1 node(s) had taints that the pod didn&#39;t tolerate.</code> <br>–&gt; 这个一般是有另一个它依赖的node还没起来，查看/var/log/messages <br>–&gt; <code>[failed to find plugin &quot;calico&quot; in path [/opt/ok8s/cni]</code> <br>–&gt; 原因是ok8s把加载的image里的默认文件挂载路径改成了<code>/opt/ok8s/cni</code>。可以直接在<code>/opt/cni/bin</code> (插件加载默认路径) 下找到<code>calico</code>和<code>calico-ipam</code>二进制文件copy到<code>/opt/ok8s/cni</code>下，也可以修改calico.yaml里的文件路径后重新apply -f<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cni]# kubectl get pods -A</span><br><span class="line">NAMESPACE     NAME                                        READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers-788d6b9876-dzlrz    1/1     Running   0          15h</span><br><span class="line">kube-system   calico-node-fttdx                           1/1     Running   0          15h</span><br><span class="line">kube-system   coredns-5644d7b6d9-dl8ft                    1/1     Running   0          15h</span><br><span class="line">kube-system   coredns-5644d7b6d9-dzrdv                    1/1     Running   0          15h</span><br><span class="line">kube-system   etcd-192-168-1-61.maas                      1/1     Running   0          15h</span><br><span class="line">kube-system   kube-apiserver-192-168-1-61.maas            1/1     Running   0          15h</span><br><span class="line">kube-system   kube-controller-manager-192-168-1-61.maas   1/1     Running   0          15h</span><br><span class="line">kube-system   kube-proxy-l54q7                            1/1     Running   0          15h</span><br><span class="line">kube-system   kube-scheduler-192-168-1-61.maas            1/1     Running   0          15h</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>modify <strong>kubelet</strong>.service <br>add <code>--feature-gates=&quot;IPv6DualStack=true&quot;</code> after <code>ExecStart=/opt/ok8s/bin/kubelet</code> in file <code>/usr/lib/systemd/system/kubelet.service</code></p>
</li>
<li><p>host server <strong>network config</strong></p>
<blockquote>
<p>refer <a href="https://www.jianshu.com/p/e92dec9f9cf4" target="_blank" rel="noopener">https://www.jianshu.com/p/e92dec9f9cf4</a></p>
</blockquote>
<p>add configs to <code>/etc/sysctl.d/98-ok8s.conf</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">net.ipv6.conf.all.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.default.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.lo.disable_ipv6 = 0</span><br><span class="line">net.ipv6.conf.all.forwarding=1</span><br></pre></td></tr></table></figure>

<p>run <code>sysctl -p</code> to make these configs work <br>enable ipv6 on host server, add setting to <code>/etc/sysconfig/network</code></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">NETWORKING_IPV6=yes</span><br></pre></td></tr></table></figure>

<p>check <code>ifcfg-xxx</code> under /etc/sysconfig/networkscripts/:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">IPV6INIT=yes</span><br><span class="line">IPV6_AUTOCONF=yes</span><br></pre></td></tr></table></figure>
</li>
<li><p>generate <strong>kubeconfig</strong> file to parse configs</p>
<blockquote>
<p>refer <a href="https://github.com/Jason-ZW/kubernetes-dual-stack-poc/blob/86f51df6766a3091fea2838416eb55dc6b83d44e/kubeadm/kubeconfig.config" target="_blank" rel="noopener">https://github.com/Jason-ZW/kubernetes-dual-stack-poc/blob/86f51df6766a3091fea2838416eb55dc6b83d44e/kubeadm/kubeconfig.config</a></p>
</blockquote>
</li>
<li><p><strong>[error]</strong> k8s v1.16.3 cannot parse subnet with comma <br><a href="https://github.com/kubernetes/kubeadm/issues/1828" target="_blank" rel="noopener">https://github.com/kubernetes/kubeadm/issues/1828</a> <br>update K8s to v1.17.4<br>packages/images updated:</p>
<ul>
<li><p>binary file: kubeadm, kubelet, kubectl, kube-proxy, kube-scheduler</p>
</li>
<li><p>docker image: controller-manager, proxy, scheduler, apiserver, etcd, coredns <br>Note: the <code>coredns</code> official package has to be loaded as <code>docker import coredns_1.6.5_linux_amd64.tgz</code> (reason see <a href="https://visionary-s.github.io/%2F2020%2F01%2F20%2Fdocker%2F">https://visionary-s.github.io/%2F2020%2F01%2F20%2Fdocker%2F</a>)<br>check package version after replace kubeadm file: <code>kubeadm config images list</code></p>
</li>
<li><p>reinstall following <a href="https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/" target="_blank" rel="noopener">official docs</a></p>
<ul>
<li><p>[error] failed to execute operation file exists when <code>systemctl enable kubelet</code> <br>solution : <code>systemctl disable kubelet</code> first, then redo enable action. <a href="https://bugs.freedesktop.org/show_bug.cgi?id=90277" target="_blank" rel="noopener">ref</a></p>
</li>
<li><p>[error] cannot use “fe80::2a80:23ff:feb5:a150” as the bind address for the API Server <br>查到kubeconfig.conf中:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">apiVersion: kubeadm.k8s.io/v1beta1</span><br><span class="line">kind: InitConfiguration</span><br><span class="line">nodeRegistration:</span><br><span class="line">  kubeletExtraArgs:</span><br><span class="line">    ##cgroup-driver: &quot;systemd&quot;</span><br><span class="line">localAPIEndpoint:</span><br><span class="line">  advertiseAddress: fe80::2a80:23ff:feb5:a150</span><br></pre></td></tr></table></figure>

<p>原因：不能用scope为link的ipv6地址，要用scope为global的<code>2001:250:4000:2000::53</code><br><img src="https://res.cloudinary.com/elizashi/image/upload/v1585103774/gitblog/posts/linux-docker-k8s/ip-scope_lajyzl.png" alt="Alt"></p>
</li>
<li><p>etcd 3.4.3 官方镜像有点问题，导致etcd启动连接老是失败<br>solution: 把老版3.1的换了个tag。。。</p>
</li>
<li><p>kube-controller-manager-192-168-1-61.maas CrashLoopBackOff <br>describe node, 发现在无限重启</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">      Warning  Unhealthy  91s                   kubelet, 192-168-1-61.maas  Liveness probe failed: Get https://127.0.0.1:10257/healthz: dial tcp 127.0.0.1:10257: connect: connection refused</span><br><span class="line">Warning  BackOff    57s (x12 over 4m30s)  kubelet, 192-168-1-61.maas  Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<p>check docker logs；</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">E0325 03:00:25.517141       1 core.go:91] Failed to start service controller: WARNING: no cloud provider provided, services of type LoadBalancer will fail</span><br><span class="line">E0325 03:00:25.638913       1 core.go:232] failed to start cloud node lifecycle controller: no cloud provider provided</span><br><span class="line">E0325 03:00:37.441597       1 controllermanager.go:521] Error starting &quot;nodeipam&quot;</span><br><span class="line">F0325 03:00:37.441623       1 controllermanager.go:235] error starting controllers: New CIDR set failed; the node CIDR size is too big</span><br></pre></td></tr></table></figure>

<p>solution: ?</p>
</li>
</ul>
</li>
<li><p>install calico</p>
<ul>
<li><p>error when starting calico node</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kube-system   calico-node-mzcnp      0/1     Init:CrashLoopBackOff   2       31s</span><br></pre></td></tr></table></figure>

<p>descirbe event:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">        Normal   Scheduled  56s                default-scheduler           Successfully assigned kube-system/calico-node-mzcnp to 192-168-1-61.maas</span><br><span class="line">Normal   Pulled     12s (x4 over 56s)  kubelet, 192-168-1-61.maas  Container image &quot;calico/cni:v3.13.1&quot; already present on machine</span><br><span class="line">Normal   Created    12s (x4 over 56s)  kubelet, 192-168-1-61.maas  Created container upgrade-ipam</span><br><span class="line">Warning  Failed     12s (x4 over 56s)  kubelet, 192-168-1-61.maas  Error: failed to start container &quot;upgrade-ipam&quot;: Error response from daemon: OCI runtime create failed: container_linux.go:346: starting container process caused &quot;exec: \&quot;/opt/ok8s/cni/calico-ipam\&quot;: stat /opt/ok8s/cni/calico-ipam: no such file or directory&quot;: unknown</span><br><span class="line">Warning  BackOff    8s (x4 over 41s)   kubelet, 192-168-1-61.maas  Back-off restarting failed container</span><br></pre></td></tr></table></figure>

<p>but I found the <code>calico-ipam</code> is already under <code>/opt/ok8s/cni/</code><br>原因：calico.yaml配置里路径写错了，按官网的来</p>
</li>
<li><p>calico node起不起来</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">      [root@192-168-1-61 calico]# kubectl get pods -A</span><br><span class="line">NAMESPACE     NAME                                        READY   STATUS              RESTARTS   AGE</span><br><span class="line">kube-system   calico-kube-controllers-788d6b9876-wkd5h    0/1     ContainerCreating   0          39m</span><br><span class="line">kube-system   calico-node-64q4h                           0/1     Running             0          59s</span><br><span class="line">kube-system   coredns-6955765f44-k9cb5                    0/1     ContainerCreating   0          45m</span><br><span class="line">kube-system   coredns-6955765f44-nznr9                    0/1     ContainerCreating   0          45m</span><br><span class="line">kube-system   etcd-192-168-1-61.maas                      1/1     Running             0          45m</span><br><span class="line">kube-system   kube-apiserver-192-168-1-61.maas            1/1     Running             0          45m</span><br><span class="line">kube-system   kube-controller-manager-192-168-1-61.maas   1/1     Running             0          45m</span><br><span class="line">kube-system   kube-proxy-ggjvb                            1/1     Running             0          45m</span><br><span class="line">kube-system   kube-scheduler-192-168-1-61.maas            1/1     Running             0          45m</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>describe calico-kube-controller:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">    Warning  FailedScheduling        59s (x30 over 39m)  default-scheduler           0/1 nodes are available: 1 node(s) had taints that the pod didn&apos;t tolerate.</span><br><span class="line">Normal   Scheduled               57s                 default-scheduler           Successfully assigned kube-system/calico-kube-controllers-788d6b9876-wkd5h to 192-168-1-61.maas</span><br><span class="line">Warning  FailedCreatePodSandBox  54s                 kubelet, 192-168-1-61.maas  Failed to create pod sandbox: rpc error: code = Unknown desc = [failed to set up sandbox container &quot;27761f7a236bf9c092826ecb546dddad7c44a40fa1891faf6c45b14f330ad25d&quot; network for pod &quot;calico-kube-controllers-788d6b9876-wkd5h&quot;: networkPlugin cni failed to set up pod &quot;calico-kube-controllers-788d6b9876-wkd5h_kube-system&quot; network: error getting ClusterInformation: Get https://[10.24.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.24.0.1:443: connect: connection refused, failed to clean up sandbox container &quot;27761f7a236bf9c092826ecb546dddad7c44a40fa1891faf6c45b14f330ad25d&quot; network for pod &quot;calico-kube-controllers-788d6b9876-wkd5h&quot;: networkPlugin cni failed to teardown pod &quot;calico-kube-controllers-788d6b9876-wkd5h_kube-system&quot; network: error getting ClusterInformation: Get https://[10.24.0.1]:443/apis/crd.projectcalico.org/v1/clusterinformations/default: dial tcp 10.24.0.1:443: connect: connection refused]</span><br><span class="line">Normal   SandboxChanged          9s (x5 over 53s)    kubelet, 192-168-1-61.maas  Pod sandbox changed, it will be killed and re-created.</span><br></pre></td></tr></table></figure>

<p>原因： <code>https://[10.24.0.1]:443</code>这个应该对应kubeconfig中kube-proxy下的clusterCIDR，应该跟kubelet下的podSubnet保持一致?（我用了serviceSubnet的网段）<br>还是不行，后面又改成了纯ipv6环境，配置文件kubeconfig.yml如下：</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">InitConfiguration</span></span><br><span class="line"><span class="attr">localAPIEndpoint:</span></span><br><span class="line"><span class="attr">  advertiseAddress:</span> <span class="number">2001</span><span class="string">:250:4000:2000::53</span></span><br><span class="line"><span class="attr">  bindPort:</span> <span class="number">6443</span></span><br><span class="line"><span class="attr">nodeRegistration:</span></span><br><span class="line"><span class="attr">  criSocket:</span> <span class="string">/var/run/dockershim.sock</span></span><br><span class="line"><span class="attr">  taints:</span></span><br><span class="line"><span class="attr">    - effect:</span> <span class="string">NoSchedule</span></span><br><span class="line"><span class="attr">      key:</span> <span class="string">node-role.kubernetes.io/master</span></span><br><span class="line"><span class="attr">  kubeletExtraArgs:</span></span><br><span class="line"><span class="attr">    node-ip:</span> <span class="string">"2001:250:4000:2000::53"</span></span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="attr">apiServer:</span></span><br><span class="line"><span class="attr">  timeoutForControlPlane:</span> <span class="number">4</span><span class="string">m0s</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeadm.k8s.io/v1beta2</span></span><br><span class="line"><span class="attr">certificatesDir:</span> <span class="string">/etc/kubernetes/pki</span></span><br><span class="line"><span class="attr">clusterName:</span> <span class="string">kubernetes</span></span><br><span class="line"><span class="attr">controlPlaneEndpoint:</span> <span class="string">"[2001:250:4000:2000::53]:6443"</span></span><br><span class="line"><span class="attr">dns:</span></span><br><span class="line"><span class="attr">  type:</span> <span class="string">CoreDNS</span></span><br><span class="line"><span class="attr">imageRepository:</span> <span class="string">k8s.gcr.io</span></span><br><span class="line"><span class="attr">controllerManager:</span></span><br><span class="line"><span class="attr">  extraArgs:</span></span><br><span class="line">    <span class="comment">#feature-gates: IPv6DualStack=true</span></span><br><span class="line"><span class="attr">    bind-address:</span> <span class="string">"::"</span></span><br><span class="line"><span class="attr">    service-cluster-ip-range:</span> <span class="attr">fd03::/120</span></span><br><span class="line"><span class="attr">    cluster-cidr:</span> <span class="string">"fd04::/120"</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterConfiguration</span></span><br><span class="line"><span class="attr">kubernetesVersion:</span> <span class="string">v1.17.4</span></span><br><span class="line"><span class="attr">networking:</span></span><br><span class="line"><span class="attr">  dnsDomain:</span> <span class="string">cluster.local</span></span><br><span class="line"><span class="attr">  serviceSubnet:</span> <span class="string">"fd03::/120"</span></span><br><span class="line"><span class="attr">  podSubnet:</span> <span class="string">"fd04::/120"</span></span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubelet.config.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeletConfiguration</span></span><br><span class="line"><span class="attr">failSwapOn:</span> <span class="literal">false</span></span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">kubeproxy.config.k8s.io/v1alpha1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">KubeProxyConfiguration</span></span><br><span class="line"><span class="attr">clusterCIDR:</span> <span class="string">"fd03::/120"</span></span><br><span class="line"><span class="attr">mode:</span> <span class="string">"ipvs"</span></span><br></pre></td></tr></table></figure>

<p>修改了<a href="https://docs.projectcalico.org/manifests/calico.yaml" target="_blank" rel="noopener">calico.yaml</a>执行脚本的参数如下：<br>！这是dual的，ipv6-only的把ipv4的部分去掉</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">kind:</span> <span class="string">ConfigMap</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">data:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  cni_network_config:</span> <span class="string">|-</span></span><br><span class="line">    <span class="string">&#123;</span></span><br><span class="line"><span class="attr">      "name":</span> <span class="string">"k8s-pod-network"</span><span class="string">,</span></span><br><span class="line"><span class="attr">      "cniVersion":</span> <span class="string">"0.3.1"</span><span class="string">,</span></span><br><span class="line"><span class="attr">      "plugins":</span> <span class="string">[</span></span><br><span class="line">        <span class="string">&#123;</span></span><br><span class="line">          <span class="string">...</span></span><br><span class="line"><span class="attr">          "ipam":</span> <span class="string">&#123;</span></span><br><span class="line"><span class="attr">            "type":</span> <span class="string">"calico-ipam"</span><span class="string">,</span></span><br><span class="line"><span class="attr">            "assign_ipv4":</span> <span class="string">"true"</span><span class="string">,</span></span><br><span class="line"><span class="attr">            "assign_ipv6":</span> <span class="string">"true"</span><span class="string">,</span></span><br><span class="line"><span class="attr">            "ipv4_pools":</span> <span class="string">["192.170.30.0/16",</span> <span class="string">"default-ipv4-ippool"</span><span class="string">],</span></span><br><span class="line"><span class="attr">            "ipv6_pools":</span> <span class="string">["fd03::/120",</span> <span class="string">"default-ipv6-ippool"</span><span class="string">]</span></span><br><span class="line">          <span class="string">&#125;,</span></span><br><span class="line">          <span class="string">...</span></span><br><span class="line">        <span class="string">&#125;,</span></span><br><span class="line">        <span class="string">...</span></span><br><span class="line">      <span class="string">]</span></span><br><span class="line">    <span class="string">&#125;</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="comment">#---</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">DaemonSet</span></span><br><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line">    <span class="string">...</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line">      <span class="string">...</span></span><br><span class="line"><span class="attr">  containers:</span></span><br><span class="line"><span class="attr">    - name:</span> <span class="string">calico-node</span></span><br><span class="line"><span class="attr">      image:</span> <span class="string">calico/node:v3.13.1</span></span><br><span class="line"><span class="attr">      env:</span></span><br><span class="line">        <span class="string">...</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">CALICO_IPV4POOL_CIDR</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"192.170.30.0/16"</span></span><br><span class="line">        <span class="string">...</span></span><br><span class="line">        <span class="comment"># Disable IPv6 on Kubernetes.</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">FELIX_IPV6SUPPORT</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"true"</span></span><br><span class="line">        <span class="string">...</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">CALICO_IPV6POOL_CIDR</span></span><br><span class="line"><span class="attr">          value:</span> <span class="attr">fd04::/120</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">IP6</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">"autodetect"</span></span><br><span class="line">        <span class="comment"># !注意，这是后来做outgress发现的，ipv6环境默认不会配网关，see</span></span><br><span class="line">        <span class="comment"># https://docs.projectcalico.org/reference/node/configuration</span></span><br><span class="line">        <span class="comment"># 所以这里要加个参数</span></span><br><span class="line"><span class="attr">        - name:</span> <span class="string">CALICO_IPV6POOL_NAT_OUTGOING</span></span><br><span class="line"><span class="attr">          value:</span> <span class="literal">true</span></span><br><span class="line">  <span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>create检查service地址能不能对上，具体debug步骤忘了，大概用了这些命令：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ss -atnlp | grep 443</span><br><span class="line">netstat -an|grep 6443</span><br></pre></td></tr></table></figure>
</li>
<li><p>又装了个Nginx pod以后，试图写一个测试service看能不能通过Nginx ping通ipv6地址，失败</p>
<ul>
<li><p>service.yaml</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Service</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">cec</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">    ipFamily:</span> <span class="string">IPv6</span></span><br><span class="line"><span class="attr">    ports:</span></span><br><span class="line"><span class="attr">      - name:</span> <span class="string">http</span></span><br><span class="line"><span class="attr">        port:</span> <span class="number">8080</span></span><br><span class="line"><span class="attr">        protocol:</span> <span class="string">TCP</span></span><br><span class="line"><span class="attr">        targetPort:</span> <span class="number">80</span></span><br><span class="line"><span class="attr">    selector:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>nginx.yaml</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">piVersion:</span> <span class="string">apps/v1</span> <span class="comment"># for versions before 1.9.0 use apps/v1beta2</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">Deployment</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">nginx-deployment</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line"><span class="attr">  selector:</span></span><br><span class="line"><span class="attr">    matchLabels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">  replicas:</span> <span class="number">2</span> <span class="comment"># tells deployment to run 2 pods matching the template</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      labels:</span></span><br><span class="line"><span class="attr">        app:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">      spec:</span></span><br><span class="line"><span class="attr">        containers:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">nginx</span></span><br><span class="line"><span class="attr">            image:</span> <span class="attr">nginx:1.14.2</span></span><br><span class="line"><span class="attr">            ports:</span></span><br><span class="line"><span class="attr">              - containerPort:</span> <span class="number">80</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>debug</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">root@192-168-1-61 ok8s]# kubectl apply -f service.yaml</span><br><span class="line">service/cec created</span><br><span class="line">[root@192-168-1-61 ok8s]# kubectl get service -A</span><br><span class="line">NAMESPACE     NAME         TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)                  AGE</span><br><span class="line">default       cec          ClusterIP   fd03::7d     &lt;none&gt;        8080/TCP                 35s</span><br><span class="line">default       kubernetes   ClusterIP   fd03::1      &lt;none&gt;        443/TCP                  4h36m</span><br><span class="line">kube-system   kube-dns     ClusterIP   fd03::a      &lt;none&gt;        53/UDP,53/TCP,9153/TCP   4h36m</span><br><span class="line">[root@192-168-1-61 ok8s]# kubectl describe -n default pod nginx-deployment-574b87c764-qptmz</span><br><span class="line">...</span><br><span class="line">Node:         192-168-1-61.maas/2001:250:4000:2000::53</span><br><span class="line">Labels:       app=nginx</span><br><span class="line">Annotations:  cni.projectcalico.org/podIP: fd04::d5/128</span><br><span class="line">              cni.projectcalico.org/podIPs: fd04::d5/128</span><br><span class="line">IP:           fd04::d5</span><br><span class="line">Containers:</span><br><span class="line">  nginx:</span><br><span class="line">    Container ID:   docker://199c74a7f44d431d95091d991025e43b24dca7fa9cc9d77e3781af1b89f160ce</span><br><span class="line">      Image:          nginx:1.14.2</span><br><span class="line">      Port:           80/TCP</span><br><span class="line">      Host Port:      0/TCP</span><br><span class="line">...</span><br><span class="line">[root@192-168-1-61 ok8s]# kubectl get pods -o yaml | grep -i podip</span><br><span class="line">    cni.projectcalico.org/podIP: fd04::d5/128</span><br><span class="line">    cni.projectcalico.org/podIPs: fd04::d5/128</span><br><span class="line">  podIP: fd04::d5</span><br><span class="line">  podIPs:</span><br><span class="line">[root@192-168-1-61 ok8s]# kubectl describe svc cec</span><br><span class="line">...</span><br><span class="line">Selector:          app=nginx</span><br><span class="line">Type:              ClusterIP</span><br><span class="line">IP:                fd03::7d</span><br><span class="line">Port:              http  8080/TCP</span><br><span class="line">TargetPort:        80/TCP</span><br><span class="line">Endpoints:         [fd04::d5]:80</span><br><span class="line">[root@192-168-1-61 ok8s]# curl -vvv -k http://[fd03::7d]:8080 -g</span><br><span class="line">* About to connect() to fd03::7d port 8080 (#0)</span><br><span class="line">*   Trying fd03::7d...</span><br><span class="line">* Connection refused</span><br><span class="line">* Failed connect to fd03::7d:8080; Connection refused</span><br><span class="line">* Closing connection 0</span><br><span class="line">curl: (7) Failed connect to fd03::7d:8080; Connection refused</span><br><span class="line">[root@192-168-1-61 ok8s]# &gt;/dev/tcp/fd03::7d/8080</span><br><span class="line">-bash: connect: Connection refused</span><br><span class="line">-bash: /dev/tcp/fd03::7d/8080: Connection refused</span><br><span class="line">[root@192-168-1-61 ok8s]# ipvsadm -ln</span><br><span class="line">IP Virtual Server version 1.2.1 (size=4096)</span><br><span class="line">Prot LocalAddress:Port Scheduler Flags</span><br><span class="line">  -&gt; RemoteAddress:Port           Forward Weight ActiveConn InActConn</span><br><span class="line">TCP  [fd03::1]:443 rr</span><br><span class="line">  -&gt; [2001:250:4000:2000::53]:6443 Masq    1      4          0         </span><br><span class="line">TCP  [fd03::a]:53 rr</span><br><span class="line">  -&gt; [fd04::cf]:53                Masq    1      0          0         </span><br><span class="line">  -&gt; [fd04::d7]:53                Masq    1      0          0         </span><br><span class="line">TCP  [fd03::a]:9153 rr</span><br><span class="line">  -&gt; [fd04::cf]:9153              Masq    1      0          0         </span><br><span class="line">  -&gt; [fd04::d7]:9153              Masq    1      0          0         </span><br><span class="line">TCP  [fd03::7d]:8080 rr</span><br><span class="line">  -&gt; [fd04::d5]:80                Masq    1      0          0         </span><br><span class="line">UDP  [fd03::a]:53 rr</span><br><span class="line">  -&gt; [fd04::cf]:53                Masq    1      0          0         </span><br><span class="line">  -&gt; [fd04::d7]:53                Masq    1      0          0</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cec-installer]# kubectl get svc,deploy</span><br><span class="line">NAME                 TYPE        CLUSTER-IP   EXTERNAL-IP   PORT(S)   AGE</span><br><span class="line">service/kubernetes   ClusterIP   fd03::1      &lt;none&gt;        443/TCP   137m</span><br><span class="line"></span><br><span class="line">NAME                               READY   UP-TO-DATE   AVAILABLE   AGE</span><br><span class="line">deployment.apps/nginx-deployment   2/2     2            2           105m</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ok8s]# kubectl exec -it nginx-deployment-574b87c764-qptmz bash</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/# &gt;/dev/tcp/127.0.0.1/80</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/# &gt;/dev/tcp/fd04::d5/80</span><br><span class="line">bash: connect: Connection refused</span><br><span class="line">bash: /dev/tcp/fd04::d5/80: Connection refused</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/# cd /etc/nginx/</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/etc/nginx# cat nginx.conf</span><br><span class="line">user  nginx;</span><br><span class="line">worker_processes  1;</span><br><span class="line"></span><br><span class="line">error_log  /var/log/nginx/error.log warn;</span><br><span class="line">pid        /var/run/nginx.pid;</span><br><span class="line"></span><br><span class="line">events &#123;</span><br><span class="line">  worker_connections  1024;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">http &#123;</span><br><span class="line">  include       /etc/nginx/mime.types;</span><br><span class="line">  default_type  application/octet-stream;</span><br><span class="line"></span><br><span class="line">  log_format  main  &apos;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &apos;</span><br><span class="line">              &apos;$status $body_bytes_sent &quot;$http_referer&quot; &apos;</span><br><span class="line">              &apos;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&apos;;</span><br><span class="line"></span><br><span class="line">  access_log  /var/log/nginx/access.log  main;</span><br><span class="line"></span><br><span class="line">  sendfile        on;</span><br><span class="line">  #tcp_nopush     on;</span><br><span class="line"></span><br><span class="line">  keepalive_timeout  65;</span><br><span class="line"></span><br><span class="line">  #gzip  on;</span><br><span class="line"></span><br><span class="line">  include /etc/nginx/conf.d/*.conf;</span><br><span class="line">&#125;</span><br><span class="line">// not modified on my desktop, thus skip the modification here</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/etc/nginx# nginx -s reload</span><br><span class="line">2020/03/26 09:01:37 [notice] 28#28: signal process started</span><br><span class="line">root@nginx-deployment-574b87c764-qptmz:/etc/nginx# &gt;/dev/tcp/fd04::d5/80</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
</li>
<li><p>install helm/tiller/nfs/ingress <br>Background: no connection to outer internet on host server, install helm and tiller locally, export tiller image and transfer it to host server.</p>
<ul>
<li><p>Step1: install helm</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">wget https://get.helm.sh/helm-v2.16.1-linux-amd64.tar.gz</span><br><span class="line">// transfer package to host server:/var/tmp/xxx-installer/</span><br><span class="line">[root@192-168-1-61 cec-installer]# tar zxvf helm-v2.16.1-linux-amd64.tar.gz</span><br><span class="line">linux-amd64/</span><br><span class="line">linux-amd64/helm</span><br><span class="line">linux-amd64/LICENSE</span><br><span class="line">linux-amd64/tiller</span><br><span class="line">linux-amd64/README.md</span><br><span class="line">[root@192-168-1-61 cec-installer]# sudo install **/helm /usr/bin</span><br><span class="line">[root@192-168-1-61 cec-installer]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.16.1&quot;, GitCommit:&quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Error: could not find tiller</span><br><span class="line">[root@192-168-1-61 cec-installer]# helm init</span><br><span class="line">Creating /root/.helm</span><br><span class="line">Creating /root/.helm/repository</span><br><span class="line">Creating /root/.helm/repository/cache</span><br><span class="line">Creating /root/.helm/repository/local</span><br><span class="line">Creating /root/.helm/plugins</span><br><span class="line">Creating /root/.helm/starters</span><br><span class="line">Creating /root/.helm/cache/archive</span><br><span class="line">Creating /root/.helm/repository/repositories.yaml</span><br><span class="line">Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">Error: error initializing: Looks like &quot;https://kubernetes-charts.storage.googleapis.com&quot; is not a valid chart repository or cannot be reached:etes-charts.storage.googleapis.com/index.yaml: dial tcp: lookup kubernetes-charts.storage.googleapis.com on 10.136.40.87:53: server misbehavi</span><br></pre></td></tr></table></figure>

<p>here refered <a href="https://www.jianshu.com/p/2bb1dfdadee8" target="_blank" rel="noopener">https://www.jianshu.com/p/2bb1dfdadee8</a></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// 这一步不知道要不要做，反正我先做了</span><br><span class="line">[root@192-168-1-61 cec-installer]# cd linux-amd64/</span><br><span class="line">[root@192-168-1-61 linux-amd64]# ls</span><br><span class="line">LICENSE  README.md  helm  tiller</span><br><span class="line">[root@192-168-1-61 linux-amd64]# vim rbac-config.yaml</span><br></pre></td></tr></table></figure>

<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">  namespace:</span> <span class="string">kube-system</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">rbac.authorization.k8s.io/v1beta1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">ClusterRoleBinding</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">roleRef:</span></span><br><span class="line"><span class="attr">  apiGroup:</span> <span class="string">rbac.authorization.k8s.io</span></span><br><span class="line"><span class="attr">  kind:</span> <span class="string">ClusterRole</span></span><br><span class="line"><span class="attr">  name:</span> <span class="string">cluster-admin</span></span><br><span class="line"><span class="attr">subjects:</span></span><br><span class="line"><span class="attr">  - kind:</span> <span class="string">ServiceAccount</span></span><br><span class="line"><span class="attr">    name:</span> <span class="string">tiller</span></span><br><span class="line"><span class="attr">    namespace:</span> <span class="string">kube-system</span></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 linux-amd64]# kubectl create -f rbac-config.yaml</span><br><span class="line">serviceaccount/tiller created</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/tiller created</span><br></pre></td></tr></table></figure>
</li>
<li><p>Step2: init tiller <br>save local <code>tiller</code> image and transfer it to host server, then follow the <a href="https://github.com/helm/helm/issues/4540" target="_blank" rel="noopener">answer@Amit-Thawait</a></p>
<blockquote>
<p>这里还遇到一个问题，本地win10的helm和tiller以前装的是2.14，这次用了2.16，不知如何升级，问了一下，原来是用2.16的helm.exe和tiller.exe文件替换掉<eid>/.helm/下的俩对应exe再init一遍就行了，估计linux上的更新就是替换一下二进制文件吧。。</eid></p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# docker image tag 1f92aa902d73 gcr.io/kubernetes-helm/tiller:v2.16.1</span><br><span class="line">[root@192-168-1-61 cec-installer]# helm init --client-only --skip-refresh</span><br><span class="line">Creating /root/.helm/repository/repositories.yaml</span><br><span class="line">Adding stable repo with URL: https://kubernetes-charts.storage.googleapis.com</span><br><span class="line">Adding local repo with URL: http://127.0.0.1:8879/charts</span><br><span class="line">$HELM_HOME has been configured at /root/.helm.</span><br><span class="line">Not installing Tiller due to &apos;client-only&apos; flag having been set</span><br><span class="line">// load tiller image</span><br><span class="line">[root@192-168-1-61 cec-installer]# helm init</span><br><span class="line">$HELM_HOME has been configured at /root/.helm.</span><br><span class="line">Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.</span><br><span class="line">Please note: by default, Tiller is deployed with an insecure &apos;allow unauthenticated users&apos; policy.</span><br><span class="line">To prevent this, run `helm init` with the --tiller-tls-verify flag.</span><br><span class="line">For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation</span><br></pre></td></tr></table></figure>

<p>helm error happens:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cec-installer]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.16.1&quot;, GitCommit:&quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">E0326 23:21:17.649683   11541 portforward.go:400] an error occurred forwarding 39127 -&gt; 44134: error forwarding port 44134 to pod 8d197e975824256d2de574f4577161702560b55a4aa70978bac91f2e43abe712, uid : unable to do port forwarding: socat not found</span><br><span class="line">E0326 23:21:18.652732   11541 portforward.go:400] an error occurred forwarding 39127 -&gt; 44134: error forwarding port 44134 to pod 8d197e975824256d2de574f4577161702560b55a4aa70978bac91f2e43abe712, uid : unable to do port forwarding: socat not found</span><br></pre></td></tr></table></figure>

<p>According to <a href="https://www.kubernetes.org.cn/3879.html" target="_blank" rel="noopener">https://www.kubernetes.org.cn/3879.html</a>, this error happens due to rr is not set to permitted in k8s net config, install socat to fix it. (internal yum repository has build up, thus download directly)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cec-installer]# yum install socat.x86_64</span><br><span class="line">[root@192-168-1-61 cec-installer]# helm version</span><br><span class="line">Client: &amp;version.Version&#123;SemVer:&quot;v2.16.1&quot;, GitCommit:&quot;bbdfe5e7803a12bbdf97e94cd847859890cf4050&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br><span class="line">Server: &amp;version.Version&#123;SemVer:&quot;v2.16.4&quot;, GitCommit:&quot;5e135cc465d4231d9bfe2c5a43fd2978ef527e83&quot;, GitTreeState:&quot;clean&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>版本好像没对上，懒得重装了凑合用吧。</p>
</li>
<li><p>Step3: install nfs <br>  <strong>[error]</strong> cannot install nfs:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">helm install stable/nfs-server-provisioner --name nfs-provisioner --set persistence.storageClass=nfs --set persistence.size=20Gi --set rbac.create=true</span><br><span class="line">Error: validation failed: [storageclasses.storage.k8s.io &quot;nfs&quot; not found, serviceaccounts &quot;nfs-provisioner-nfs-server-provisioner&quot; not found, clusterroles.rbac.authorization.k8s.io &quot;nfs-provisioner-nfs-server-provisioner&quot; not found, clusterrolebindings.rbac.authorization.k8s.io &quot;nfs-provisioner-nfs-server-provisioner&quot; not found, services &quot;nfs-provisioner-nfs-server-provisioner&quot; not found, statefulsets.apps &quot;nfs-provisioner-nfs-server-provisioner&quot; not found]</span><br></pre></td></tr></table></figure>

<p> try to downgrade tiller to 2.16.1 on windows:</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ helm reset --force</span><br><span class="line">$ helm init -i registry.cn-hangzhou.aliyuncs.com/google_containers/tiller:v2.16.1</span><br></pre></td></tr></table></figure>

<p> nfs installed on windows successfully:</p>
 <figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get storageclass</span><br><span class="line">NAME                 PROVISIONER                                            AGE</span><br><span class="line">hostpath (default)   docker.io/hostpath                                     7d3h</span><br><span class="line">nfs                  cluster.local/nfs-provisioner-nfs-server-provisioner   102s</span><br></pre></td></tr></table></figure>

<p> for offline linux server, firstly download <a href="https://github.com/helm/charts/tree/master/stable/nfs-server-provisioner" target="_blank" rel="noopener">helm chart package</a>, then package the chart files to tgz, helm install it on host server:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cec-installer]# helm install -n nfs-provisioner ./nfs-servier-provisoner.tgz \</span><br><span class="line">&gt; --set persistence.storageClass=nfs \</span><br><span class="line">&gt; --set persistence.size=20Gi</span><br><span class="line"></span><br><span class="line">Error: release nfs-provisioner failed: namespaces &quot;default&quot; is forbidden: User &quot;system:serviceaccount:kube-system:default&quot; cannot get resource &quot;namespaces&quot; in API group &quot;&quot; in the namespace &quot;default&quot;</span><br></pre></td></tr></table></figure>

<p> 应该是helm创建sa时未添加helm-tiller，按这个<a href="https://forum.choerodon.io/t/topic/1721/3" target="_blank" rel="noopener">步骤</a>加上就ok了:</p>
 <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 cec-installer]# kubectl create serviceaccount --namespace kube-system helm-tiller</span><br><span class="line">serviceaccount/helm-tiller created</span><br><span class="line"></span><br><span class="line">[root@192-168-1-61 cec-installer]# kubectl create clusterrolebinding helm-tiller-cluster-rule --clusterrole=cluster-admin --serviceaccount=kube-system:helm-tiller</span><br><span class="line">clusterrolebinding.rbac.authorization.k8s.io/helm-tiller-cluster-rule created</span><br><span class="line"></span><br><span class="line">[root@192-168-1-61 cec-installer]# helm init --service-account=helm-tiller --upgrade</span><br><span class="line">$HELM_HOME has been configured at /root/.helm.</span><br><span class="line">Tiller (the Helm server-side component) has been updated to gcr.io/kubernetes-helm/tiller:v2.16.1 .</span><br><span class="line"></span><br><span class="line">[root@192-168-1-61 cec-installer]# helm install -n nfs-provisioner ./nfs-servier-provisoner.tgz --set persistence.storageClass=nfs --set persistence.size=20Gi</span><br><span class="line">NAME:   nfs-provisioner</span><br><span class="line">LAST DEPLOYED: Mon Mar 30 02:13:19 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line">...</span><br><span class="line">[root@192-168-1-61 cec-installer]# kubectl get storageclass</span><br><span class="line">NAME   PROVISIONER                                            RECLAIMPOLICY   VOLUMEBINDINGMODE   ALLOWVOLUMEEXPANSION   AGE</span><br><span class="line">nfs    cluster.local/nfs-provisioner-nfs-server-provisioner   Delete          Immediate           true                   2m42s</span><br></pre></td></tr></table></figure>
</li>
<li><p>Step4: install nginx-ingress, similar to installing nfs</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docker load -i ingress-nginx.tar.gz</span><br><span class="line">docker image tag 20c7790fd73d quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.29.0</span><br><span class="line">helm install -n nginx-ingress ./nginx-ingress-deploy.tgz --set rbac.create=true</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>finally, start to install our product</p>
<ul>
<li><p>产品jenkins-ci因为没人维护挂了。。只好本地打包</p>
<ul>
<li><p>step1: <code>./startindesignenv.sh -c -d</code> -d步骤中最后为<code>npm start</code>所以最后不会停要手动cancel进程。build + deploy完了以后可以在coreservice/backend/webroot（类似target）下找到拷贝过去的前端编译文件。</p>
</li>
<li><p>step2: 在任一service目录下（例：coreservice）<code>$ docker build -t &lt;package-name&gt; .</code>执行打包，coreservice比较特殊，因为前后端一体所以要执行第一步先把前端整合到后端，其他service直接打包即可。</p>
<ul>
<li>error1:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Step 1/15 : FROM node:10.15.3-alpine</span><br><span class="line">error pulling image configuration: Get https://registry-1.docker.io/v2/library/node/blobs.. net/http: TLS handshake timeout</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>solution : network not stable, ignore and build again</p>
<ul>
<li>error2:<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"> Step 2/15 : RUN apk add --no-cache bash openssl</span><br><span class="line"> fetch http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz</span><br><span class="line"> WARNING: Ignoring http://dl-cdn.alpinelinux.org/alpine/v3.9/main/x86_64/APKINDEX.tar.gz: network error (check Internet connection and firewall)</span><br><span class="line"> ERROR: unsatisfiable constraints:</span><br><span class="line">    bash (missing):</span><br><span class="line">      required by: world[bash]</span><br><span class="line">    openssl (missing):</span><br><span class="line">      required by: world[openssl]</span><br><span class="line">The <span class="built_in">command</span> <span class="string">'/bin/sh -c apk add --no-cache bash openssl'</span> returned a non-zero code: 2</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>solution: didnt figure out the detailed reason, but restart docker cannot solve this issue. Thanks to Ye who taught me to add</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ENV</span> HTTP_PROXY=<span class="string">"http://www-proxy.lmera.ericsson.se:8080"</span></span><br><span class="line"><span class="keyword">ENV</span> HTTPS_PROXY=<span class="string">"http://www-proxy.lmera.ericsson.se:8080"</span></span><br></pre></td></tr></table></figure>

<p>to Dockerfile, the problem due to no connection to external link while running the <code>alpinelinux</code>.</p>
<blockquote>
<p>the proxy ENV config will also take effect on container running on host server after loading, we would better use <code>--build-arg</code> when execute <code>docker build</code>, see <a href="https://www.cntofu.com/book/139/image/dockerfile/arg.md" target="_blank" rel="noopener">https://www.cntofu.com/book/139/image/dockerfile/arg.md</a><br>e.g. $ docker build -t core-cec . –build-arg HTTP_PROXY=”<a href="http://www-proxy.lmera.ericsson.se:8080&quot;" target="_blank" rel="noopener">http://www-proxy.lmera.ericsson.se:8080&quot;</a></p>
</blockquote>
</li>
<li><p>Step3: install on linux server</p>
<ol>
<li><code>docker load -i &lt;package&gt;</code></li>
<li><code>helm install -n eri2 ./cec-release-1.0.0.tgz --set service.type=NodePort --set persistence.enabled=false --set persistence.storageClass=nfs --set persistence.database.size=5Gi --set persistence.miscellaneous.size=5Gi</code></li>
<li>deployed but got error, when building one service container, run image as container and enter to check:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">  [root@192-168-1-61 cec-installer]# docker run -it registry/cec-sai:1.0.0 sh</span><br><span class="line">  /opt/ericsson/cec/saiservice # /usr/local/bin/npm start</span><br><span class="line">  /opt/ericsson/cec/saiservice/node_modules/bindings/bindings.js:91</span><br><span class="line">    throw e</span><br><span class="line">    ^</span><br><span class="line">    Error: Error loading shared library /opt/ericsson/cec/saiservice/node_modules/libxmljs/build/Release/xmljs.node: Exec format error</span><br><span class="line">at Object.Module._extensions..node (internal/modules/cjs/loader.js:730:18)</span><br><span class="line">at Module.load (internal/modules/cjs/loader.js:600:32)</span><br><span class="line">at tryModuleLoad (internal/modules/cjs/loader.js:539:12)</span><br><span class="line">at Function.Module._load (internal/modules/cjs/loader.js:531:3)</span><br><span class="line">at Module.require (internal/modules/cjs/loader.js:637:17)</span><br><span class="line">at require (internal/modules/cjs/helpers.js:22:18)</span><br><span class="line">at bindings (/opt/ericsson/cec/saiservice/node_modules/bindings/bindings.js:84:48)</span><br><span class="line">at Object.&lt;anonymous&gt; (/opt/ericsson/cec/saiservice/node_modules/libxmljs/lib/bindings.js:1:99)</span><br><span class="line">at Module._compile (internal/modules/cjs/loader.js:701:30)</span><br><span class="line">at Object.Module._extensions..js (internal/modules/cjs/loader.js:712:10)</span><br><span class="line">npm ERR! code ELIFECYCLE</span><br><span class="line">npm ERR! errno 1</span><br><span class="line">npm ERR! CEC_SAI_Handler@1.0.0 start: `node server.js`</span><br><span class="line">npm ERR! Exit status 1</span><br><span class="line">npm ERR!</span><br><span class="line">npm ERR! Failed at the CEC_SAI_Handler@1.0.0 start script.</span><br><span class="line">npm ERR! This is probably not a problem with npm. There is likely additional logging output above.</span><br><span class="line">npm ERR! A complete log of this run can be found in:</span><br><span class="line">npm ERR!     /root/.npm/_logs/2020-03-31T02_30_46_424Z-debug.log</span><br></pre></td></tr></table></figure>

</li>
</ol>
<p>it may due to the some nodes in <code>npm_modules</code> not work both on windows and linux env. ref: <a href="https://dzone.com/articles/packaging-a-node-app-for-docker-from-windows" target="_blank" rel="noopener">https://dzone.com/articles/packaging-a-node-app-for-docker-from-windows</a> <br>solution: remove <code>node_modules</code> before docker image build, npm install all packages in docker container, thus it will be generated in linux format.</p>
<figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line"><span class="keyword">COPY</span><span class="bash"> . ./</span></span><br><span class="line"><span class="keyword">RUN</span><span class="bash"> rm -rf node_modules</span></span><br><span class="line">...</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure>

<p>build and check image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker build -t cec-sai . --build-arg HTTP_PROXY=<span class="string">"http://www-proxy.lmera.ericsson.se:8080"</span></span><br><span class="line">$ winpty docker run -it cec-sai sh</span><br><span class="line">/opt/ericsson/cec/saiservice <span class="comment"># find / -name npm</span></span><br><span class="line">/usr/<span class="built_in">local</span>/lib/node_modules/npm</span><br><span class="line">/usr/<span class="built_in">local</span>/lib/node_modules/npm/bin/npm</span><br><span class="line">/usr/<span class="built_in">local</span>/bin/npm</span><br><span class="line">/opt/ericsson/cec/saiservice <span class="comment"># /usr/local/bin/npm start</span></span><br><span class="line">&gt; CEC_SAI_Handler@1.0.0 start /opt/ericsson/cec/saiservice</span><br><span class="line">&gt; node server.js</span><br><span class="line">sequelize deprecated String based operators are now deprecated. Please use Symbol based operators <span class="keyword">for</span> better security, <span class="built_in">read</span> more at http://docs.sequelizejs.com/manual/tutorial/querying.html<span class="comment">#operators node_modules/sequelize/lib/sequelize.js:245:13</span></span><br></pre></td></tr></table></figure>

<p>save image:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker save -o cec-sai-200327.tar.gz cec-sai</span><br></pre></td></tr></table></figure>

<p>then load it to host server on linux</p>
<blockquote>
<p>Run enm server via docker container is a simpler way, use <code>docker run -itd --name &lt;simulator-name&gt; -p 8091:8091 -v &lt;volume-path&gt; &lt;image-id or name&gt;</code>, volune path can be skipped to set, remind to expose port 8091 or the container cannot be accessed. (port depends on your design)</p>
</blockquote>
</li>
<li><p>Step4: install simulator <br>save image on windows, load it to linux, then start simulator service on linux server:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl create deployment xxx-simulator --image=xxx-simulator:1.0.0</span><br><span class="line">deployment.apps/xxx-simulator created</span><br></pre></td></tr></table></figure>

<p>get simulator ip address:</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ kubectl get pod -l app=enm-simulator -o wide</span><br><span class="line">NAME                             READY   STATUS    RESTARTS   AGE    IP         NODE                NOMINATED NODE   READINESS GATES</span><br><span class="line">xxx-simulator-79d68f588f-8l2fq   1/1     Running   0          4m3s   fd04::fb   192-168-1-61.maas   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>Test functionalities</p>
<ul>
<li><p>[error1] connot apply ipv6 address in our settings. <br>solution: change validation regex from <code>^http(s)?://[\\w\\.\\-?=%&amp;:/]+$</code> to <code>^http(s)?://([\[\\w\\.\\-?=%&amp;:/](\])?)+$</code></p>
</li>
<li><p>[error2] cannot connect to simulator when do cell import:<br>solution: modify default <code>HOST</code> config in <code>server.js</code> from ‘0.0.0.0’ to ‘’. see <a href="https://nodejs.org/api/net.html#net_net_isip_input" target="_blank" rel="noopener">official node.js docs</a> section = <em>server.listen(options[, callback])</em> <br>here I record my debug history:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ~]# kubectl exec -it xxx-simulator-79d68f588f-8l2fq bash</span><br><span class="line">bash-4.4# netstat -tunlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address           State       PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:8091            0.0.0.0:*               LISTEN      23/node</span><br><span class="line">tcp        0      0 0.0.0.0:8092            0.0.0.0:*               LISTEN      23/node</span><br></pre></td></tr></table></figure>

<p>the ip is deafult in ipv4 format, so enter our core service (frontend and backend) pod to check more.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ~]# kubectl exec -it -c xxx-core eri1-xxx-6c4b88cd75-7plb8 bash</span><br><span class="line">bash-4.4# ping6 fd04::fd</span><br><span class="line">PING fd04::fd (fd04::fd): 56 data bytes</span><br><span class="line">64 bytes from fd04::fd: seq=0 ttl=63 time=0.144 ms</span><br><span class="line">64 bytes from fd04::fd: seq=1 ttl=63 time=0.179 ms</span><br><span class="line">64 bytes from fd04::fd: seq=2 ttl=63 time=0.069 ms</span><br></pre></td></tr></table></figure>

<p>fd04::fd is the ipv6 address of simulator, so simulator can be accessed by core service.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bash-4.4# netstat -tunlp</span><br><span class="line">Active Internet connections (only servers)</span><br><span class="line">Proto Recv-Q Send-Q Local Address           Foreign Address         State       PID/Program name    </span><br><span class="line">tcp        0      0 0.0.0.0:5432            0.0.0.0:*               LISTEN      -</span><br><span class="line">tcp        0      0 :::8082                 :::*                    LISTEN      -</span><br><span class="line">tcp        0      0 :::8083                 :::*                    LISTEN      -</span><br><span class="line">tcp        0      0 :::8084                 :::*                    LISTEN      -</span><br><span class="line">tcp        0      0 :::8888                 :::*                    LISTEN      16/node</span><br><span class="line">tcp        0      0 :::5432                 :::*                    LISTEN      -</span><br><span class="line">tcp        0      0 :::8443                 :::*                    LISTEN      16/node</span><br><span class="line">tcp        0      0 :::8585                 :::*                    LISTEN      16/node</span><br></pre></td></tr></table></figure>

<p>ports are all correct, so there might be errors in simulator codes</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@192-168-1-61 ~]# kubectl exec -it enm-simulator-77c8886b88-7ksxk bash</span><br><span class="line">bash-4.4# grep -i host /opt/eri../xxx/simulatorservice/server.js</span><br><span class="line">const HOST = &apos;0.0.0.0&apos;;</span><br><span class="line">httpServer.listen(HTTP_PORT, HOST, () =&gt; &#123;</span><br><span class="line">  LOGGER.info(&quot;HTTP Server is running on : http://%s:%s&quot;, HOST, HTTP_PORT);</span><br><span class="line">httpsServer.listen(HTTPS_PORT, HOST, () =&gt; &#123;</span><br><span class="line">  ...</span><br></pre></td></tr></table></figure>

<p>果然。。。config default HOST = ‘0.0.0.0’ –&gt; HOST = ‘’</p>
</li>
<li><p>system error : <code>connect ECONNREFUSED 127.0.0.1:8091,ENM=http://[fd04::fe]:8091</code> <br>this error indicate the default route the simulator service connect to is not correct, check in codes I found:</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">module</span>.exports = &#123;</span><br><span class="line">  ENMSERVICEPROTOCO: process.env.enmserviceprotoco || <span class="string">'http:'</span>,</span><br><span class="line">  ENMSERVICEPORT: process.env.enmserviceport || <span class="number">8083</span>,</span><br><span class="line">  ENMSERVICEHOST: process.env.enmservicehost || <span class="string">''</span>,</span><br><span class="line"></span><br><span class="line">  ENMNODEPROTOCO: process.env.enmnodeprotocol || <span class="string">'http:'</span>,</span><br><span class="line">  ENMNODEPORT: process.env.enmnodeport || <span class="number">8091</span>,</span><br><span class="line">  ENMNODEHOST: process.env.enmnodehost || <span class="string">'localhost'</span>,</span><br><span class="line"></span><br><span class="line">  CORENODEPROTOCO: process.env.corenodeprotocol || <span class="string">'http:'</span>,</span><br><span class="line">  CORENODEPORT: process.env.corenodeport || <span class="number">8585</span>,</span><br><span class="line">  CORENODEHOST: process.env.corenodehost || <span class="string">'localhost'</span>,</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>I guess there should be a <code>ENV</code> parameter configured in xxx simulator deployment.yaml. To temporarily fix this problem, I edit the deployment config of xxx-simulator, refer <a href="https://stackoverflow.com/questions/49928819/how-to-pull-environment-variables-with-helm-charts" target="_blank" rel="noopener">answer</a> and <a href="https://kubernetes.io/docs/tasks/inject-data-application/define-environment-variable-container/" target="_blank" rel="noopener">k8s official doc</a>:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">...</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="string">...</span></span><br><span class="line"><span class="attr">  template:</span></span><br><span class="line"><span class="attr">    metadata:</span></span><br><span class="line"><span class="attr">      creationTimestamp:</span> <span class="literal">null</span></span><br><span class="line"><span class="attr">    labels:</span></span><br><span class="line"><span class="attr">      app:</span> <span class="string">xxx-simulator</span></span><br><span class="line"><span class="attr">    spec:</span></span><br><span class="line"><span class="attr">      containers:</span></span><br><span class="line"><span class="attr">        - env:</span></span><br><span class="line"><span class="attr">          - name:</span> <span class="string">enmnodehost</span></span><br><span class="line"><span class="attr">          value:</span> <span class="string">'[fd04::fe]'</span></span><br><span class="line"><span class="attr">          image:</span> <span class="attr">xxx-simulator:1.0.0</span></span><br><span class="line"><span class="string">...</span></span><br></pre></td></tr></table></figure>

<p>这方法不对，原因：<code>connect ECONNREFUSED 127.0.0.1:8091</code>是enmservice和simulator在通信，这个相当于连了localhost:8091 (查看enmservice的log也确实如此)，理论上应连[fd04::fe]:8091。问题出在enmservice request export of simulator的地址，所以改simulator的container启动配置没用。<br>检查simulator的export代码，发现当时写的时候没考虑到ipv6，因为我们的create完job以后从simulator export的jobUrl是这么传的。。</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">const</span> fqdn = req.headers &amp;&amp; req.headers[<span class="string">"host"</span>] ? req.headers[<span class="string">"host"</span>].split(<span class="string">':'</span>)[<span class="number">0</span>] : <span class="literal">null</span>;</span><br><span class="line">JOBSDATA = <span class="keyword">new</span> JOBS(moJobId, fqdn);</span><br></pre></td></tr></table></figure>

<p>所以error message里会出现<code>http://[fd04:8091/bulk-con...</code>这种url</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"message"</span>: <span class="string">"Error: connect ECONNREFUSED ::1:8091"</span>,</span><br><span class="line"><span class="string">"options"</span>: &#123;</span><br><span class="line">  <span class="string">"ca"</span>: [</span><br><span class="line">    <span class="string">""</span></span><br><span class="line">  ],</span><br><span class="line">  <span class="string">"jar"</span>: &#123;&#125;,</span><br><span class="line">  <span class="string">"url"</span>: <span class="string">"http://[fd04:8091/bulk-configuration/v1/import-jobs/jobs/18974277/files"</span>,</span><br><span class="line">  <span class="string">"method"</span>: <span class="string">"POST"</span></span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>改了一下</p>
<figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">var</span> fqdn = <span class="string">""</span>;</span><br><span class="line">    <span class="keyword">if</span> (req.headers &amp;&amp; req.headers[<span class="string">"host"</span>]) &#123;</span><br><span class="line">        <span class="keyword">const</span> headerHost = req.headers[<span class="string">"host"</span>];</span><br><span class="line">        <span class="keyword">if</span> (<span class="regexp">/\[/</span>.test(headerHost)) &#123;</span><br><span class="line">            <span class="keyword">let</span> start = headerHost.indexOf(<span class="string">'['</span>);</span><br><span class="line">            <span class="keyword">let</span> end = headerHost.indexOf(<span class="string">']'</span>);</span><br><span class="line">            fqdn = headerHost.slice(start, end + <span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            fqdn = headerHost.split(<span class="string">':'</span>)[<span class="number">0</span>];</span><br><span class="line">        &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        fqdn = <span class="literal">null</span>;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://visionary-s.github.io/2020/03/20/k8s-ipv6-dualstack-support/" data-id="ckcvsvttv005igwhrodk0bi1u"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/">Kubernetes</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/k8s/">k8s</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/04/07/Kubernetes/" class="article-nav-link">
        <strong class="article-nav-caption">Newer posts</strong>
        <div class="article-nav-title">
          
            K8s - kubectl
          
        </div>
      </a>
    
    
      <a href="/2020/02/11/shell/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">shell grammar</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span> visitors</li>
  
</ul>

    </div>
    <ul class="list-inline">
      <li>&copy; 2020 B. Eliza Shi</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>

<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/ming.svg" alt="Solito"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>

<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
    <i class="fe fe-rocket"></i>
</div>
    </li>
    <!-- <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li> -->
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" href="https://github.com/visionary-s" title="GitHub">
        <i class="fe fe-github"></i>
      </a>
    </li>
  </ul>
</nav>

<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>

</aside>
<script src="/js/jquery-2.0.3.min.js"></script>
<script src="/js/lazyload.min.js"></script>
<script src="/js/snap.svg-min.js"></script>
<script src="/js/busuanzi-2.3.pure.min.js"></script>


  <script src="/fancybox/jquery.fancybox.min.js"></script>



  <script src="/js/search.js"></script>


<script src="/js/ocean.js"></script>

</body>
</html>