<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  
  
  
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <title>
    Errors encountered in using k8s |
    
    Solito</title>
  
    <link rel="shortcut icon" href="/favicon.ico">
  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
  
<script src="/js/pace.min.js"></script>

<meta name="generator" content="Hexo 5.3.0"></head>

<body>
<main class="content">
  <section class="outer">
  <article id="post-k8s-errors" class="article article-type-post" itemscope itemprop="blogPost" data-scroll-reveal>

  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Errors encountered in using k8s
    </h1>
  


      </header>
    

    
      <div class="article-meta">
        <a href="/2020/04/08/k8s-errors/" class="article-date">
  <time datetime="2020-04-07T16:00:00.000Z" itemprop="datePublished">2020-04-08</time>
</a>
        
  <div class="article-category">
    <a class="article-category-link" href="/categories/work/">work</a>
  </div>

      </div>
    

    <div class="article-entry" itemprop="articleBody">
      

      

      
        <p>This is a record in deploying k8s in work stage.</p>
<h2 id="Issue-1-failed-to-schedule-pod-for-not-running-“VolumeBinding”-filter-plugin"><a href="#Issue-1-failed-to-schedule-pod-for-not-running-“VolumeBinding”-filter-plugin" class="headerlink" title="Issue 1: failed to schedule pod for not running “VolumeBinding” filter plugin"></a>Issue 1: failed to schedule pod for not running “VolumeBinding” filter plugin</h2><p>describe pod:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                 From               Message</span><br><span class="line">  ----     ------            ----                ----               -------</span><br><span class="line">  Warning  FailedScheduling  55s (x19 over 26m)  default-scheduler  error while running &quot;VolumeBinding&quot; filter plugin for pod &quot;eri-cec-9dcd4d6c8-whvrm&quot;: pod has unbound immediate PersistentVolumeClaims</span><br></pre></td></tr></table></figure>
<p>check pod specs by <code>kubectl edit pod &lt;pod-name&gt;</code>:</p>
<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">volumes:</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cec-database</span></span><br><span class="line">  <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">    <span class="attr">claimName:</span> <span class="string">eri-cec-database-pvc</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">cec-misc</span></span><br><span class="line">  <span class="attr">persistentVolumeClaim:</span></span><br><span class="line">    <span class="attr">claimName:</span> <span class="string">eri-cec-misc-pvc</span></span><br><span class="line"><span class="bullet">-</span> <span class="attr">name:</span> <span class="string">default-token-pxxg8</span></span><br><span class="line">  <span class="attr">secret:</span></span><br><span class="line">    <span class="attr">defaultMode:</span> <span class="number">420</span></span><br><span class="line">    <span class="attr">secretName:</span> <span class="string">default-token-pxxg8</span></span><br><span class="line"><span class="attr">status:</span></span><br><span class="line">  <span class="attr">conditions:</span></span><br><span class="line">  <span class="bullet">-</span> <span class="attr">lastProbeTime:</span> <span class="literal">null</span></span><br><span class="line">    <span class="attr">lastTransitionTime:</span> <span class="string">&quot;2020-04-08T01:58:02Z&quot;</span></span><br><span class="line">    <span class="attr">message:</span> <span class="string">&#x27;error while running &quot;VolumeBinding&quot; filter plugin for pod &quot;eri-cec-9dcd4d6c8-whvrm&quot;:</span></span><br><span class="line"><span class="string">      pod has unbound immediate PersistentVolumeClaims&#x27;</span></span><br><span class="line">    <span class="attr">reason:</span> <span class="string">Unschedulable</span></span><br><span class="line">    <span class="attr">status:</span> <span class="string">&quot;False&quot;</span></span><br><span class="line">    <span class="attr">type:</span> <span class="string">PodScheduled</span></span><br><span class="line">  <span class="attr">phase:</span> <span class="string">Pending</span></span><br><span class="line">  <span class="attr">qosClass:</span> <span class="string">BestEffort</span></span><br></pre></td></tr></table></figure>
<p>The two persistentVolumeClaims were set during <code>helm install</code>:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">helm install -n eri .&#x2F;cec-release-1.0.0.tgz \</span><br><span class="line">&gt; --set persistence.enabled&#x3D;true \</span><br><span class="line">&gt; --set persistence.storageClass&#x3D;nfs \</span><br><span class="line">&gt; --set persistence.database.size&#x3D;5Gi \</span><br><span class="line">&gt; --set persistence.miscellaneous.size&#x3D;5Gi \</span><br><span class="line">&gt; --set ingress.cecManager.hostName&#x3D;dual-test \</span><br><span class="line">&gt; --set ingress.cecApi.hostName&#x3D;dual-api</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">NAME:   eri</span><br><span class="line">LAST DEPLOYED: Wed Apr  8 09:58:00 2020</span><br><span class="line">NAMESPACE: default</span><br><span class="line">STATUS: DEPLOYED</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">RESOURCES:</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Deployment</span><br><span class="line">NAME     AGE</span><br><span class="line">eri-cec  1s</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;PersistentVolumeClaim</span><br><span class="line">NAME                  AGE</span><br><span class="line">eri-cec-database-pvc  2s</span><br><span class="line">eri-cec-misc-pvc      2s</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Pod(related)</span><br><span class="line">NAME                     AGE</span><br><span class="line">eri-cec-9dcd4d6c8-whvrm  1s</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Secret</span><br><span class="line">NAME                     AGE</span><br><span class="line">eri-cec-database-secret  2s</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D;&#x3D;&gt; v1&#x2F;Service</span><br><span class="line">NAME     AGE</span><br><span class="line">eri-cec  1s</span><br><span class="line">&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;&#x3D;</span><br><span class="line">&#x3D;&#x3D;&gt; v1beta1&#x2F;Ingress</span><br><span class="line">NAME             AGE</span><br><span class="line">eri-cec-ingress  1s</span><br></pre></td></tr></table></figure>
<p>=========================solution========================= <br>Refers:</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/60774220/kubernetes-pod-has-unbound-immediate-persistentvolumeclaims">https://stackoverflow.com/questions/60774220/kubernetes-pod-has-unbound-immediate-persistentvolumeclaims</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/oguro/article/details/96964440">https://blog.csdn.net/oguro/article/details/96964440</a></li>
<li><a target="_blank" rel="noopener" href="https://blog.csdn.net/liumiaocn/article/details/103388607">https://blog.csdn.net/liumiaocn/article/details/103388607</a></li>
<li><a target="_blank" rel="noopener" href="https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/">https://kubernetes.io/docs/tasks/configure-pod-container/configure-persistent-volume-storage/</a> <br>From above info, there is something wrong with PVC (PersistentVolumeClaims), which leaves state “unbound”. The PVC should be bound to certain PV (PersistentVolume) which has enough capacity to hold the binding PVC. <br>check PVCs and PVs:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl get pvc -A</span><br><span class="line">NAMESPACE   NAME                      STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE</span><br><span class="line">cec         eri-sh-cec-database-pvc   Pending                                      nfs            125m</span><br><span class="line">cec         eri-sh-cec-misc-pvc       Pending                                      nfs            125m</span><br><span class="line">default     eri-cec-database-pvc      Pending                                      nfs            3h22m</span><br><span class="line">default     eri-cec-misc-pvc          Pending                                      nfs            3h22m</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl get pv -A</span><br><span class="line">No resources found</span><br></pre></td></tr></table></figure>
There is no PV on current node-63, thus I create two PVs for db-pvc and misc-pvc.<br>Make a directory for PVs first:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 mnt]# sudo mkdir &#x2F;mnt&#x2F;data</span><br></pre></td></tr></table></figure>
check permission and capacity the PVC need:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl edit pvc eri-sh-cec-database-pvc -n cec</span><br><span class="line">...</span><br><span class="line">accessModes:</span><br><span class="line">  - ReadWriteOnce</span><br><span class="line">resources:</span><br><span class="line">    requests:</span><br><span class="line">      storage: 5Gi</span><br></pre></td></tr></table></figure>
vi pv-init.yaml:<figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eri-sh-cec-database-pv</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">eri-sh-cec-database-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/mnt/data</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">nfs</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteMany&quot;</span>,<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br><span class="line"><span class="meta">---</span></span><br><span class="line"><span class="attr">apiVersion:</span> <span class="string">v1</span></span><br><span class="line"><span class="attr">kind:</span> <span class="string">PersistentVolume</span></span><br><span class="line"><span class="attr">metadata:</span></span><br><span class="line">  <span class="attr">name:</span> <span class="string">eri-sh-cec-misc-pv</span></span><br><span class="line">  <span class="attr">labels:</span></span><br><span class="line">    <span class="attr">name:</span> <span class="string">eri-sh-cec-misc-pv</span></span><br><span class="line"><span class="attr">spec:</span></span><br><span class="line">  <span class="attr">nfs:</span></span><br><span class="line">    <span class="attr">path:</span> <span class="string">/mnt/data</span></span><br><span class="line">    <span class="attr">server:</span> <span class="string">nfs</span></span><br><span class="line">  <span class="attr">accessModes:</span> [<span class="string">&quot;ReadWriteMany&quot;</span>,<span class="string">&quot;ReadWriteOnce&quot;</span>]</span><br><span class="line">  <span class="attr">capacity:</span></span><br><span class="line">    <span class="attr">storage:</span> <span class="string">5Gi</span></span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl apply -f pv-init.yaml</span><br><span class="line">persistentvolume&#x2F;eri-sh-cec-database-pv created</span><br><span class="line">persistentvolume&#x2F;eri-sh-cec-misc-pv created</span><br><span class="line">[root@host63 cec-installer]# kubectl get pv -A</span><br><span class="line">NAME                     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE</span><br><span class="line">eri-sh-cec-database-pv   5Gi        RWO,RWX        Retain           Available                                   15s</span><br><span class="line">eri-sh-cec-misc-pv       5Gi        RWO,RWX        Retain           Available                                   15s</span><br></pre></td></tr></table></figure>
PVC still not work, check its status:<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl describe pvc eri-sh-cec-database-pvc -n cec</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                Age                   From                                                                                                                                Message</span><br><span class="line">  ----     ------                ----                  ----                                                                                                                                -------</span><br><span class="line">  Warning  ProvisioningFailed    54m (x3 over 104m)    cluster.local&#x2F;nfs-provisioner-nfs-server-provisioner_nfs-provisioner-nfs-server-provisioner-0_79370fad-78b7-11ea-8b82-66556e93189d  failed to provision volume with StorageClass &quot;nfs&quot;: error getting NFS server IP for volume: service SERVICE_NAME&#x3D;nfs-provisioner-nfs-server-provisioner is not valid; check that it has for ports map[&#123;111 UDP&#125;:true &#123;111 TCP&#125;:true &#123;2049 TCP&#125;:true &#123;20048 TCP&#125;:true] exactly one endpoint, this pod&#39;s IP POD_IP&#x3D;192.168.220.144</span><br><span class="line">  Warning  ProvisioningFailed    38m (x8 over 3h6m)    cluster.local&#x2F;nfs-provisioner-nfs-server-provisioner_nfs-provisioner-nfs-server-provisioner-0_79370fad-78b7-11ea-8b82-66556e93189d  failed to provision volume with StorageClass &quot;nfs&quot;: error getting NFS server IP for volume: service SERVICE_NAME&#x3D;nfs-provisioner-nfs-server-provisioner is not valid; check that it has for ports map[&#123;2049 TCP&#125;:true &#123;20048 TCP&#125;:true &#123;111 UDP&#125;:true &#123;111 TCP&#125;:true] exactly one endpoint, this pod&#39;s IP POD_IP&#x3D;192.168.220.144</span><br><span class="line">  Normal   Provisioning          21m (x16 over 3h6m)   cluster.local&#x2F;nfs-provisioner-nfs-server-provisioner_nfs-provisioner-nfs-server-provisioner-0_79370fad-78b7-11ea-8b82-66556e93189d  External provisioner is provisioning volume for claim &quot;cec&#x2F;eri-sh-cec-database-pvc&quot;</span><br><span class="line">  Warning  ProvisioningFailed    21m (x2 over 3h4m)    cluster.local&#x2F;nfs-provisioner-nfs-server-provisioner_nfs-provisioner-nfs-server-provisioner-0_79370fad-78b7-11ea-8b82-66556e93189d  failed to provision volume with StorageClass &quot;nfs&quot;: error getting NFS server IP for volume: service SERVICE_NAME&#x3D;nfs-provisioner-nfs-server-provisioner is not valid; check that it has for ports map[&#123;111 TCP&#125;:true &#123;2049 TCP&#125;:true &#123;20048 TCP&#125;:true &#123;111 UDP&#125;:true] exactly one endpoint, this pod&#39;s IP POD_IP&#x3D;192.168.220.144</span><br><span class="line">  Normal   ExternalProvisioning  87s (x742 over 3h6m)  persistentvolume-controller                                                                                                         waiting for a volume to be created, either by external provisioner &quot;cluster.local&#x2F;nfs-provisioner-nfs-server-provisioner&quot; or manually created by system administrator</span><br></pre></td></tr></table></figure>
<strong>Did not figure out</strong></li>
</ul>
<hr>
<hr>
<h2 id="Issue-2-The-deployed-pod-without-nfs-would-use-ipv4-as-default-and-service-nginx-ingress-will-use-ipv4-address-as-default-route-cannot-change-it"><a href="#Issue-2-The-deployed-pod-without-nfs-would-use-ipv4-as-default-and-service-nginx-ingress-will-use-ipv4-address-as-default-route-cannot-change-it" class="headerlink" title="Issue 2. The deployed pod (without nfs) would use ipv4 as default, and service nginx-ingress will use ipv4 address as default route, cannot change it."></a>Issue 2. The deployed pod (without nfs) would use ipv4 as default, and service <code>nginx-ingress</code> will use ipv4 address as default route, cannot change it.</h2><p>log nginx-ingress:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# kubectl logs nginx-ingress-controller-64d58897bd-b99gw</span><br><span class="line">  -------------------------------------------------------------------------------</span><br><span class="line">  NGINX Ingress controller</span><br><span class="line">    Release:       0.29.0</span><br><span class="line">    Build:         git-eedcdcdbf</span><br><span class="line">    Repository:    https:&#x2F;&#x2F;github.com&#x2F;kubernetes&#x2F;ingress-nginx</span><br><span class="line">    nginx version: nginx&#x2F;1.17.8</span><br><span class="line">  -------------------------------------------------------------------------------</span><br><span class="line">  I0407 10:17:14.810155       8 flags.go:215] Watching for Ingress class: nginx</span><br><span class="line">  W0407 10:17:14.811042       8 flags.go:260] SSL certificate chain completion is disabled (--enable-ssl-chain-completion&#x3D;false)</span><br><span class="line">  W0407 10:17:14.811123       8 client_config.go:543] Neither --kubeconfig nor --master was specified.  Using the inClusterConfig.  This might not work.</span><br><span class="line">  I0407 10:17:14.811367       8 main.go:193] Creating API client for https:&#x2F;&#x2F;192.167.0.1:443</span><br><span class="line">  I0407 10:17:14.820212       8 main.go:237] Running in Kubernetes cluster version v1.17 (v1.17.4) - git (clean) commit 8d8aa39598534325ad77120c120a22b3a990b5ea - platform linux&#x2F;amd64</span><br><span class="line">  I0407 10:17:14.823302       8 main.go:91] Validated default&#x2F;nginx-ingress-default-backend as the default backend.</span><br><span class="line">  I0407 10:17:15.126113       8 main.go:102] SSL fake certificate created &#x2F;etc&#x2F;ingress-controller&#x2F;ssl&#x2F;default-fake-certificate.pem</span><br><span class="line">  W0407 10:17:15.147723       8 store.go:657] Unexpected error reading configuration configmap: configmaps &quot;nginx-ingress-controller&quot; not found</span><br><span class="line">  I0407 10:17:15.156374       8 nginx.go:263] Starting NGINX Ingress controller</span><br><span class="line">  I0407 10:17:16.357204       8 nginx.go:307] Starting NGINX process</span><br><span class="line">  I0407 10:17:16.357338       8 leaderelection.go:242] attempting to acquire leader lease  default&#x2F;ingress-controller-leader-nginx...</span><br><span class="line">  W0407 10:17:16.358186       8 controller.go:394] Service &quot;default&#x2F;nginx-ingress-default-backend&quot; does not have any active Endpoint</span><br><span class="line">  I0407 10:17:16.358304       8 controller.go:137] Configuration changes detected, backend reload required.</span><br><span class="line">  I0407 10:17:16.360127       8 status.go:86] new leader elected: nginx-ingress-controller-64d58897bd-cthrs</span><br><span class="line">  I0407 10:17:16.450895       8 controller.go:153] Backend successfully reloaded.</span><br><span class="line">  I0407 10:17:16.450966       8 controller.go:162] Initial sync, sleeping for 1 second.</span><br><span class="line">  W0407 10:17:20.280746       8 controller.go:394] Service &quot;default&#x2F;nginx-ingress-default-backend&quot; does not have any active Endpoint</span><br><span class="line">  W0407 10:17:23.614240       8 controller.go:394] Service &quot;default&#x2F;nginx-ingress-default-backend&quot; does not have any active Endpoint</span><br><span class="line">  W0407 10:17:33.458971       8 controller.go:394] Service &quot;default&#x2F;nginx-ingress-default-backend&quot; does not have any active Endpoint</span><br><span class="line">  I0407 10:17:53.811527       8 leaderelection.go:252] successfully acquired lease default&#x2F;ingress-controller-leader-nginx</span><br><span class="line">  I0407 10:17:53.811566       8 status.go:86] new leader elected: nginx-ingress-controller-64d58897bd-b99gw</span><br><span class="line">  W0407 10:18:00.868971       8 controller.go:394] Service &quot;default&#x2F;nginx-ingress-default-backend&quot; does not have any active Endpoint</span><br><span class="line">  I0408 03:14:30.743173       8 event.go:281] Event(v1.ObjectReference&#123;Kind:&quot;Ingress&quot;, Namespace:&quot;cec&quot;, Name:&quot;eri-sh-cec-ingress&quot;, UID:&quot;dd298fd3-3c16-42e8-a544-c7f942ec4e3e&quot;, APIVersion:&quot;networking.k8s.io&#x2F;v1beta1&quot;, ResourceVersion:&quot;211359&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;CREATE&#39; Ingress cec&#x2F;eri-sh-cec-ingress</span><br><span class="line">  W0408 03:14:34.068588       8 controller.go:921] Service &quot;default&#x2F;eri-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:34.068631       8 controller.go:921] Service &quot;default&#x2F;eri-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:34.068648       8 controller.go:921] Service &quot;cec&#x2F;eri-sh-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:34.068661       8 controller.go:921] Service &quot;cec&#x2F;eri-sh-cec&quot; does not have any active Endpoint.</span><br><span class="line">  I0408 03:14:53.817883       8 status.go:274] updating Ingress cec&#x2F;eri-sh-cec-ingress status from [] to [&#123;10.136.40.63 &#125;]</span><br><span class="line">  I0408 03:14:53.820045       8 event.go:281] Event(v1.ObjectReference&#123;Kind:&quot;Ingress&quot;, Namespace:&quot;cec&quot;, Name:&quot;eri-sh-cec-ingress&quot;, UID:&quot;dd298fd3-3c16-42e8-a544-c7f942ec4e3e&quot;, APIVersion:&quot;networking.k8s.io&#x2F;v1beta1&quot;, ResourceVersion:&quot;211445&quot;, FieldPath:&quot;&quot;&#125;): type: &#39;Normal&#39; reason: &#39;UPDATE&#39; Ingress cec&#x2F;eri-sh-cec-ingress</span><br><span class="line">  W0408 03:14:53.820285       8 controller.go:921] Service &quot;default&#x2F;eri-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:53.820310       8 controller.go:921] Service &quot;default&#x2F;eri-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:53.820326       8 controller.go:921] Service &quot;cec&#x2F;eri-sh-cec&quot; does not have any active Endpoint.</span><br><span class="line">  W0408 03:14:53.820341       8 controller.go:921] Service &quot;cec&#x2F;eri-sh-cec&quot; does not have any active Endpoint.</span><br></pre></td></tr></table></figure>
<p>Solved the problem of product service using default ipv4 as cluster-ip by adding new paras in helm deployment charts:</p>
<ul>
<li><code>ipFamily:</code> below <code>service:</code> in <code>values.yaml</code></li>
<li><pre><code class="yaml"><span class="attr">ipFamily:</span> <span class="string">&#123;&#123;.Values.service.ipFamily&#125;&#125;</span>
<span class="string"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">  below &#96;spec:&#96; in &#96;service.yaml&#96;</span><br><span class="line">  - &#96;--set service.ipFamily&#x3D;IPv6&#96; when helm install product</span><br><span class="line"></span><br><span class="line">**Solution:**</span><br><span class="line">Modify helm chart before installing ingress, in &#96;value.yaml&#96;, config:</span><br><span class="line">&#96;&#96;&#96;yaml</span><br><span class="line">hostNetwork: true</span><br><span class="line">reportNodeInternalIp: true</span><br><span class="line">daemonset:</span><br><span class="line">  useHostPort: true</span><br><span class="line">kind: DaemonSet</span><br></pre></td></tr></table></figure></span>
<span class="string">you</span> <span class="string">can</span> <span class="string">also</span> <span class="string">set</span> <span class="string">service</span> <span class="string">type</span> <span class="string">and</span> <span class="string">external_IP</span> <span class="string">here.</span>
<span class="string">After</span> <span class="string">helm</span> <span class="string">install,</span> <span class="string">check</span> <span class="string">if</span> <span class="string">you</span> <span class="string">can</span> <span class="string">visit</span> <span class="string">service</span> <span class="string">using</span> <span class="string">hostname</span> <span class="string">via</span> <span class="string">host-ip.</span>
<span class="string"><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">[root@host63 cec-installer]# curl http:&#x2F;&#x2F;dual-ipv6:80</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;html&gt;</span><br><span class="line">[root@host63 cec-installer]# curl http:&#x2F;&#x2F;dual-ipv4:80</span><br><span class="line">&lt;!DOCTYPE html&gt;</span><br><span class="line">&lt;html&gt;</span><br><span class="line">...</span><br><span class="line">&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure></span>
<span class="string">also</span> <span class="string">check</span> <span class="string">with</span> <span class="string">client</span> <span class="string">UI.</span>
</code></pre>
</li>
</ul>
<hr>
<hr>
<h2 id="Issue-3-Failed-to-init-tiller-pod-creating-error-rpc-error-code-DeadlineExceeded-desc-context-deadline-exceeded"><a href="#Issue-3-Failed-to-init-tiller-pod-creating-error-rpc-error-code-DeadlineExceeded-desc-context-deadline-exceeded" class="headerlink" title="Issue 3. Failed to init tiller, pod creating error: rpc error: code = DeadlineExceeded desc = context deadline exceeded"></a>Issue 3. Failed to init tiller, pod creating error: rpc error: code = DeadlineExceeded desc = context deadline exceeded</h2><p>pod hang on state <code>ContainerCreating</code> after doing helm init. describe pod:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">[root@host59 ~]# kubectl describe -n kube-system pod tiller-deploy-969865475-sn2k2</span><br><span class="line">Name:           tiller-deploy-969865475-sn2k2</span><br><span class="line">Namespace:      kube-system</span><br><span class="line">Node:           host59&#x2F;2001:1b74:88:9400::59:59</span><br><span class="line">Controlled By:  ReplicaSet&#x2F;tiller-deploy-969865475</span><br><span class="line">Containers:</span><br><span class="line">  tiller:</span><br><span class="line">    Container ID:   </span><br><span class="line">    Image:          gcr.io&#x2F;kubernetes-helm&#x2F;tiller:v2.16.1</span><br><span class="line">    Image ID:       </span><br><span class="line">    Ports:          44134&#x2F;TCP, 44135&#x2F;TCP</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason                  Age                   From               Message</span><br><span class="line">  Normal   Scheduled               54m                   default-scheduler  Successfully assigned kube-system&#x2F;tiller-deploy-969865475-sn2k2 to host59</span><br><span class="line">  Warning  FailedCreatePodSandBox  2m47s (x13 over 50m)  kubelet, host59    Failed to create pod sandbox: rpc error: code &#x3D; DeadlineExceeded desc &#x3D; context deadline exceeded</span><br><span class="line">  Normal   SandboxChanged          2m47s (x13 over 50m)  kubelet, host59    Pod sandbox changed, it will be killed and re-created.</span><br></pre></td></tr></table></figure>
<p>check docker containers, which is already running, so there must be something wrong with docker</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[root@host59 ~]# systemctl status kubelet -l</span><br><span class="line">Apr 17 17:25:27 host59 kubelet[19205]: E0417 17:25:27.660517   19205 dns.go:135] Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 10.221.16.11 10.221.16.10 150.236.34.180</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[root@host59 ~]# journalctl -u  kubelet -f</span><br><span class="line">Apr 17 17:27:18 host59 kubelet[19205]: E0417 17:27:18.262418   19205 cni.go:385] Error deleting kube-system_tiller-deploy-969865475-sn2k2&#x2F;f35df2a630d07b0ec7149fb06d7216c60a3c77a7118924c7b7eb9556b02f5cab from network multus&#x2F;multus-cni-network: netplugin failed with no error message</span><br><span class="line">Apr 17 17:27:18 host59 kubelet[19205]: W0417 17:27:18.263092   19205 cni.go:331] CNI failed to retrieve network namespace path: Error: No such container: beb6e83c61bc47ba808dcc51e6c76e89817efb1f518fe28bc1083c99ad4721e1</span><br><span class="line">Apr 17 17:27:19 host59 kubelet[19205]: E0417 17:27:19.660435   19205 dns.go:135] Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 10.221.16.11 10.221.16.10 150.236.34.180</span><br></pre></td></tr></table></figure>
<p>So the <code>multus</code> pod/container is not running correctly. Check the pod:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[root@host59 ~]# kubectl describe pod  -n kube-system pod kube-multus-ds-amd64-wz5xj</span><br><span class="line">Name:         kube-multus-ds-amd64-wz5xj</span><br><span class="line">Namespace:    kube-system</span><br><span class="line">Node:         host59&#x2F;2001:1b74:88:9400::59:59</span><br><span class="line">Events:</span><br><span class="line">  Type     Reason            Age                    From             Message</span><br><span class="line">  Warning  DNSConfigForming  117s (x291 over 6h7m)  kubelet, host59  Nameserver limits were exceeded, some nameservers have been omitted, the applied nameserver line is: 10.221.16.11 10.221.16.10 150.236.34.180</span><br><span class="line">Error from server (NotFound): pods &quot;pod&quot; not found</span><br></pre></td></tr></table></figure>
<p>Delete pod kube-mutlus:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kubectl delete -f multus-daemonset.yml</span><br></pre></td></tr></table></figure>
<p>still not work, edit deployment find:</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">[root@host59 opt]# kubectl edit deploy tiller-deploy -n kube-system</span><br><span class="line">status:</span><br><span class="line">  conditions:</span><br><span class="line">  - lastTransitionTime: &quot;2020-04-20T01:53:55Z&quot;</span><br><span class="line">    lastUpdateTime: &quot;2020-04-20T01:53:55Z&quot;</span><br><span class="line">    message: Deployment does not have minimum availability.</span><br><span class="line">    reason: MinimumReplicasUnavailable</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: Available</span><br><span class="line">  - lastTransitionTime: &quot;2020-04-20T02:03:56Z&quot;</span><br><span class="line">    lastUpdateTime: &quot;2020-04-20T02:03:56Z&quot;</span><br><span class="line">    message: ReplicaSet &quot;tiller-deploy-b747845f&quot; has timed out progressing.</span><br><span class="line">    reason: ProgressDeadlineExceeded</span><br><span class="line">    status: &quot;False&quot;</span><br><span class="line">    type: Progressing</span><br><span class="line">  observedGeneration: 1</span><br><span class="line">  replicas: 1</span><br><span class="line">  unavailableReplicas: 1</span><br><span class="line">  updatedReplicas: 1</span><br></pre></td></tr></table></figure>
<p>Finally find the reason: mutlus is not compatible with calico, thus <code>Error deleting kube-system_tiller-deploy... from network multus/multus-cni-network: netplugin failed with no error message</code> happened as above. Even if I delete mutlus before, it has already been configured in etcd config file under /etc/kubernetes. So modify related config and the tiller pod will turn to normal.</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="https://visionary-s.github.io/2020/04/08/k8s-errors/" data-id="ckk7xapst006l48hr2tgk9tm8"
         class="article-share-link">Share</a>
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/K8s/" rel="tag">K8s</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Kubernetes/" rel="tag">Kubernetes</a></li></ul>

    </footer>

  </div>

  
    
  <nav class="article-nav">
    
      <a href="/2020/05/09/postgres-error-linux/" class="article-nav-link">
        <strong class="article-nav-caption">Newer posts</strong>
        <div class="article-nav-title">
          
            Failed starting postgres DB container via k8s on linux
          
        </div>
      </a>
    
    
      <a href="/2020/04/07/Kubernetes/" class="article-nav-link">
        <strong class="article-nav-caption">Olde posts</strong>
        <div class="article-nav-title">K8s - kubectl</div>
      </a>
    
  </nav>


  

  
    
  

</article>



</section>
  <footer class="footer">
  <div class="outer">
    <div class="float-right">
      <ul class="list-inline">
  
    <li><i class="fe fe-smile-alt"></i> <span id="busuanzi_value_site_uv"></span> visitors</li>
  
</ul>

    </div>
    <ul class="list-inline">
      <li>&copy; 2021 B. Eliza Shi</li>
      <li>Powered by <a href="http://hexo.io/" target="_blank">Hexo</a></li>
      <li>Theme  <a target="_blank" rel="noopener" href="https://github.com/zhwangart/hexo-theme-ocean">Ocean</a></li>
    </ul>
  </div>
</footer>

</main>
<aside class="sidebar">
  <button class="navbar-toggle"></button>

<nav class="navbar">
  
    <div class="logo">
      <a href="/"><img src="/images/ming.svg" alt="Solito"></a>
    </div>
  
  <ul class="nav nav-main">
    
      <li class="nav-item">
        <a class="nav-item-link" href="/">Home</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/archives">Archives</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/gallery">Gallery</a>
      </li>
    
      <li class="nav-item">
        <a class="nav-item-link" href="/about">About</a>
      </li>
    
    <li class="nav-item">
      <a class="nav-item-link nav-item-search" title="Search">
        <i class="fe fe-search"></i>
        Search
      </a>
    </li>
  </ul>
</nav>

<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      <div class="totop" id="totop">
    <i class="fe fe-rocket"></i>
</div>
    </li>
    <!-- <li class="nav-item">
      
        <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
          <i class="fe fe-feed"></i>
        </a>
      
    </li> -->
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" href="https://github.com/visionary-s" title="GitHub">
        <i class="fe fe-github"></i>
      </a>
    </li>
  </ul>
</nav>

<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>

</aside>

<script src="/js/jquery-2.0.3.min.js"></script>


<script src="/js/lazyload.min.js"></script>


<script src="/js/snap.svg-min.js"></script>


<script src="/js/busuanzi-2.3.pure.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>






<script src="/js/ocean.js"></script>


</body>
</html>